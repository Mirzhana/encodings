## Юникод - глобальный стандарт по поддержке всех языков мира

[EN](https://mirzhana.github.io/encodings/index_eng)   [GitHub](https://github.com/Mirzhana/encodings/)

### До появления **Unicode**

В компьютерных системах, вся информация обрабатывается и сохраняется процессором в
виде набора цифр. Изначально, символы в компьютерных системах были в виде бинарного кода и занимали 8 бит в памяти компьютера. Для облегчения написания бинарного кода, была разработана специальная кодировка ASCII для латинского алфавита, которая включала в себя 128 символов (прописные и строчные буквы(A-Z, a-z), числа(0-9) и технические символы). Кодировка ASCII была самодостаточна только для англоговорящих пользователей и не охватывала другие языки. Поэтому, пользователи начали создавать собственные локальные кодировки. 
Вместе с новыми кодировками появились и проблемы совместимости (одно и тоже число могло представлять разные символы в разных кодировках). С появлением интернета это привело к частому повреждению данных при их передачи между компьютерами и даже программами.


 ASCII| Binary    | Символ
------| ---------| ----
65| 01000001|A
74| 01001010 |J
46| 00101110 |.


### Появление **Unicode**

Глобальный стандарт кодировки Unicode был создан для решения проблем с совместимостью кодировок и расширением используемых символов. 

Unicode представляет каждому символу уникальную последовательность цифр (битов) вне зависимости от языка, страны, платформы и программы. Благодаря данному стандарту, сегодня в Интернете используется множество языков, пользователи и программисты могут создавать контент и информационные системы на своих родных языках. 

Важно понимать, что Unicode это стандарт который определяет принадлежность определенного символа к определенному набору цифр. Если же говорить о способах сохранения и передачи символов, то это определяется кодировками на базе стандарта Unicode.

### Кодировки **Unicode**

Разобравшись в актуальности использования стандарта Unicode, программисты обнаруживают что есть несколько популярных способов кодировки. Наиболее часто встречаемые форматы кодировок в основе Unicode это UTF-8 и UTF-16.

UTF-8 и UTF-16 различаются между собой количеством байтов, используемых при кодировке символов. Максимально оба формата могут использовать до 4 байтов(например, для вывода Эмоджи). Минимально же, UTF-8 использует 1 байт(например, для вывода буквы "О"), в то время как UTF-16 использует 2 байта минимум.

### Сравнение **UTF-8** и **UTF-16**

_Совместимость с кодировкой  ASCII_
Так как ASCII код появился гораздо раньше Unicode стандарта, многие данные все еще закодированы с помощью ASCII. Как было указано в предыдущем разделе, символ закодированный с помощью ASCII занимает 8 бит в памяти компьютера, равно как и UTF-8. В свою очередь UTF-16, при работе с ASCII кодировкой, будет иметь по 8 пустых битов, так как минимально рассчитан на 16 бит. 

_Занимаемая память_
Файл закодированный с помощью формата UTF-8 занимает в два раза меньше памяти компьютера, чем файл в кодировке UTF-16. 

_Символы высокого порядка_
Зачастую, некоторые локальные и не широко распространенные символы занимают меньше памяти в кодировке UTF-16, нежели в кодировке UTF-8. То есть, в некоторых случаях UTF-8 может выделять 4 байта на кодировку символа в то время как UTF-16 выделяет минимальные 2 байта. Для большинства данных и программ использующих азиатские языки и иероглифы, UTF-16 более эффективен.

_Примеры_

![alt text](https://github.com/Mirzhana/encodings/blob/master/img/example.jpg?raw=true " ")



### Сравнительная таблица кодировок **UTF-8** и **UTF-16**
 ASCII             | UTF-8    | UTF-16
-------------------| ---------| ----
Используемая память| 8 bit |16 bit
Совместимость ASCII| Лучше |Хуже
Символы высокого порядка| Хуже |Лучше



### Внутренняя кодировка у разных языков программирования
 Язык             | Кодировка   
-------------------| ---------
C/C++| UTF-8
C#		|UTF-16
Go	|	UTF-8
Java	|	 UTF-16
JavaScript |	Нет внутренней кодировки
Kotlin	|	UTF-8
PHP		| ASCII
Python	| Выбирается наименее затратная
Swift	|	UTF-8



